{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input, Hidden Layer, Output Layer\n",
    "# Neuron\n",
    "# Activation Fucntion\n",
    "# How do neural network work\n",
    "# how do Neural network learn\n",
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neuron , Dendrites, Axon, Synapse(Weights), Receptor, NeuraTransmiter\n",
    "\n",
    "# Neuron(Node) get input layer(neuron)\n",
    "\n",
    "# What happened in Neuron (Multiply the weight and variable and Sum all the input)\n",
    "\n",
    "# Activation Function - Threshold\n",
    "# X-axis Weighted Sum\n",
    "# Y - Axis Values 0 to 1\n",
    "# 0 or 1 based on weighted Sum\n",
    "\n",
    "# Activation Function - Sigmoid \n",
    "# X-axis Weighted Sum\n",
    "# Y - Axis Values 0 to 1\n",
    "# Smooth\n",
    "\n",
    "\n",
    "# Activation Function - Rectifier\n",
    "# X-axis Weighted Sum\n",
    "# Y - Axis Values # max(x,0)\n",
    "\n",
    "\n",
    "\n",
    "# Activation Function - Hyperbolic Tangent\n",
    "# X-axis Weighted Sum\n",
    "# Y - Axis Values -1 to 1\n",
    "\n",
    "#Xavier Glorot Rectified book\n",
    "\n",
    "# Hidden Layer we appliy Rectified and Output layer we apply sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How neural Network work and Learn\n",
    "\n",
    "# Example of Relu is Age of house 0-100 is 0 and 100 more is 1\n",
    "# Y^ is output value , Y is Actual Value\n",
    "# Cost function 1/2(y^-y)2 - Error in Prediction\n",
    "\n",
    "# Only Control to the Neural network is weight, it will be updated based on Cost function\n",
    "#X1,x2,XM input is looking at 3 values in row, and try to predict fourth value (Exam result)\n",
    "# x1 - Study Hrs - 12, x2 -Sleep Hrs - 6, x3 - Quiz -78%\n",
    "\n",
    "# 1 Epoc is go through whole dataset\n",
    "\n",
    "# Cross Validated - Reading - stackexchange list of cost exchange\n",
    "\n",
    "# Gradient descent - How weight get changed based on error\n",
    "# Very simple - Pecerptor - Single forward Neural network\n",
    "\n",
    "# Approach 1 - for grdient descent - Brute force to minimize the cost function.- Convex\n",
    "# Batch Gradient Descent, Deterministic Alogorithm\n",
    "# Curse of Dimensionality\n",
    "\n",
    "# Stochastic Gradient Descent - Non Convex\n",
    "# Take one row , Run a Network, Adjust the weight - \n",
    "# Reading Andrew Trask (15)\n",
    "# Michael Nielson - neural networks and deep la=\n",
    "\n",
    "# Backpropogation\n",
    "# During the process of BP, We are able to adjust the weight of all nodes\n",
    "\n",
    "\n",
    "\n",
    "#Training the ANN with Stochastic Gradient Descent\n",
    "\n",
    "#Step1 : Randomly intialise the weights to small numbers close to 0 (but not 0)\n",
    "\n",
    "#Step 2 : Input the first observation of your dataset in the input layer, each feature in one input node.\n",
    "\n",
    "#Step 3: Forward Propogration : from left to right the neurons are activated in a way the impact of each neurons activation\n",
    "#    is limited by  thr weights. Propogate the activations until getting the precited result Y^\n",
    "\n",
    "# Step 4 : Compare the predicted result to the actual result, Measure the generated error.\n",
    "\n",
    "# Step 5 : Backpropogation : from right to left the error is back propogated, update the weights according to\n",
    "#   how much they are responsible for the error, The learning rate decided how much we update the weights\n",
    "\n",
    "# Step 6 : Repeat 1 to 5 , Update the weights after each observation(Reinforcement learning) or\n",
    "# Repeat 1 to 5 but update the weights only after  a batch of observation (Batch learning)\n",
    "\n",
    "# Step 7 : When the who training set passed through the ANN, that makes an epoch, Redo more epochs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano\n",
    "# Theano Numerical computation library can run also in GPU\n",
    "# GPU is much more computation and process more parallel and Floating Point operations(FLOPS)\n",
    "# Developed by University of Montreal\n",
    "\n",
    "# Tensor Flow - Developed by google deepmind\n",
    "# Keras - wraps both Tensorflow and Theano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayyanar\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>15574012</td>\n",
       "      <td>Chu</td>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>15592531</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>822</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>15656148</td>\n",
       "      <td>Obinna</td>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>15792365</td>\n",
       "      <td>He</td>\n",
       "      <td>501</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>15592389</td>\n",
       "      <td>H?</td>\n",
       "      <td>684</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>15767821</td>\n",
       "      <td>Bearce</td>\n",
       "      <td>528</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>102016.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80181.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>15737173</td>\n",
       "      <td>Andrews</td>\n",
       "      <td>497</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76390.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>15632264</td>\n",
       "      <td>Kay</td>\n",
       "      <td>476</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26260.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>15691483</td>\n",
       "      <td>Chin</td>\n",
       "      <td>549</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190857.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15600882</td>\n",
       "      <td>Scott</td>\n",
       "      <td>635</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65951.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>15643966</td>\n",
       "      <td>Goforth</td>\n",
       "      <td>616</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>143129.41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64327.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>15737452</td>\n",
       "      <td>Romeo</td>\n",
       "      <td>653</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>15788218</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>549</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14406.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>15661507</td>\n",
       "      <td>Muldrow</td>\n",
       "      <td>587</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158684.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>15568982</td>\n",
       "      <td>Hao</td>\n",
       "      <td>726</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54724.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
       "0           1    15634602   Hargrave          619    France  Female   42   \n",
       "1           2    15647311       Hill          608     Spain  Female   41   \n",
       "2           3    15619304       Onio          502    France  Female   42   \n",
       "3           4    15701354       Boni          699    France  Female   39   \n",
       "4           5    15737888   Mitchell          850     Spain  Female   43   \n",
       "5           6    15574012        Chu          645     Spain    Male   44   \n",
       "6           7    15592531   Bartlett          822    France    Male   50   \n",
       "7           8    15656148     Obinna          376   Germany  Female   29   \n",
       "8           9    15792365         He          501    France    Male   44   \n",
       "9          10    15592389         H?          684    France    Male   27   \n",
       "10         11    15767821     Bearce          528    France    Male   31   \n",
       "11         12    15737173    Andrews          497     Spain    Male   24   \n",
       "12         13    15632264        Kay          476    France  Female   34   \n",
       "13         14    15691483       Chin          549    France  Female   25   \n",
       "14         15    15600882      Scott          635     Spain  Female   35   \n",
       "15         16    15643966    Goforth          616   Germany    Male   45   \n",
       "16         17    15737452      Romeo          653   Germany    Male   58   \n",
       "17         18    15788218  Henderson          549     Spain  Female   24   \n",
       "18         19    15661507    Muldrow          587     Spain    Male   45   \n",
       "19         20    15568982        Hao          726    France  Female   24   \n",
       "\n",
       "    Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0        2       0.00              1          1               1   \n",
       "1        1   83807.86              1          0               1   \n",
       "2        8  159660.80              3          1               0   \n",
       "3        1       0.00              2          0               0   \n",
       "4        2  125510.82              1          1               1   \n",
       "5        8  113755.78              2          1               0   \n",
       "6        7       0.00              2          1               1   \n",
       "7        4  115046.74              4          1               0   \n",
       "8        4  142051.07              2          0               1   \n",
       "9        2  134603.88              1          1               1   \n",
       "10       6  102016.72              2          0               0   \n",
       "11       3       0.00              2          1               0   \n",
       "12      10       0.00              2          1               0   \n",
       "13       5       0.00              2          0               0   \n",
       "14       7       0.00              2          1               1   \n",
       "15       3  143129.41              2          0               1   \n",
       "16       1  132602.88              1          1               0   \n",
       "17       9       0.00              2          1               1   \n",
       "18       6       0.00              1          0               0   \n",
       "19       6       0.00              2          1               1   \n",
       "\n",
       "    EstimatedSalary  Exited  \n",
       "0         101348.88       1  \n",
       "1         112542.58       0  \n",
       "2         113931.57       1  \n",
       "3          93826.63       0  \n",
       "4          79084.10       0  \n",
       "5         149756.71       1  \n",
       "6          10062.80       0  \n",
       "7         119346.88       1  \n",
       "8          74940.50       0  \n",
       "9          71725.73       0  \n",
       "10         80181.12       0  \n",
       "11         76390.01       0  \n",
       "12         26260.98       0  \n",
       "13        190857.79       0  \n",
       "14         65951.65       0  \n",
       "15         64327.26       0  \n",
       "16          5097.67       1  \n",
       "17         14406.41       0  \n",
       "18        158684.81       0  \n",
       "19         54724.03       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2037"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[:,-1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',\n",
       "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
       "       'IsActiveMember', 'EstimatedSalary', 'Exited'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the independend variable impact on Churn or predict value Y^\n",
    "# Select the dataset\n",
    "X = dataset.iloc[:,3:13].values\n",
    "y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
       "       ..., \n",
       "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
       "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
       "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the encode the categorical variable lies on independed variable X ( Country and Gender)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "label_encoder_X_1 = LabelEncoder()\n",
    "X[:,1] = label_encoder_X_1.fit_transform(X[:,1])\n",
    "label_encoder_X_2 = LabelEncoder()\n",
    "X[:,2] = label_encoder_X_2.fit_transform(X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to create a dummy variable for Country since we have three country, No need for gender since we have only two variable\n",
    "\n",
    "onehot_encoder = OneHotEncoder(categorical_features=[1])\n",
    "X = onehot_encoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove the extra dummy variable \n",
    "X = X[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into Training set and Testing Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to apply Feature scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5698444 ,  1.74309049,  0.16958176, ...,  0.64259497,\n",
       "        -1.03227043,  1.10643166],\n",
       "       [ 1.75486502, -0.57369368, -2.30455945, ...,  0.64259497,\n",
       "         0.9687384 , -0.74866447],\n",
       "       [-0.5698444 , -0.57369368, -1.19119591, ...,  0.64259497,\n",
       "        -1.03227043,  1.48533467],\n",
       "       ..., \n",
       "       [-0.5698444 , -0.57369368,  0.9015152 , ...,  0.64259497,\n",
       "        -1.03227043,  1.41231994],\n",
       "       [-0.5698444 ,  1.74309049, -0.62420521, ...,  0.64259497,\n",
       "         0.9687384 ,  0.84432121],\n",
       "       [ 1.75486502, -0.57369368, -0.28401079, ...,  0.64259497,\n",
       "        -1.03227043,  0.32472465]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.75486502, -0.57369368, -0.55204276, -1.09168714, -0.36890377,\n",
       "        1.04473698,  0.8793029 , -0.92159124,  0.64259497,  0.9687384 ,\n",
       "        1.61085707])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - Letz do the ANN\n",
    "\n",
    "# Import the keras libraries and packages\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the models\n",
    "#Module to Sequencial Model - Intialize the Neural netwoek\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Module to Dense - Build the Layer of ANN\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intializing the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of independed Variable (Column)(input_dim) in X_test is : -------> 11\n",
      "Output Dim is half of input variable in X_test is : -------> 5.5\n"
     ]
    }
   ],
   "source": [
    "#len(X_test[0])\n",
    "\n",
    "print (\"The number of independed Variable (Column)(input_dim) in X_test is : -------> {}\".format(len(X_test[0])))\n",
    "print (\"Output Dim is half of input variable in X_test is : -------> {}\".format(len(X_test[0])/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayyanar\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=11, activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Build the Input layer and first Hidden layer\n",
    "#https://keras.io/getting-started/sequential-model-guide/\n",
    "classifier.add(Dense(input_dim = 11, output_dim = 6, init = 'uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayyanar\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Add another hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayyanar\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Add  final and output layer - Since the output is binary outcome , so we have only on enode to give a result 1 or 0\n",
    "# If you need probabilty - Use Sigmoid function\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the ANN\n",
    "# https://keras.io/getting-started/sequential-model-guide/#compilation\n",
    "# https://keras.io/optimizers/#adam for \n",
    "#https://keras.io/losses/#binary_crossentropy\n",
    "classifier.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayyanar\\Anaconda3\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 4s 543us/step - loss: 0.4879 - acc: 0.8014\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 3s 388us/step - loss: 0.4059 - acc: 0.8239\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 3s 391us/step - loss: 0.3922 - acc: 0.8284\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 3s 432us/step - loss: 0.3839 - acc: 0.8289\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 3s 378us/step - loss: 0.3779 - acc: 0.8331\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 3s 372us/step - loss: 0.3730 - acc: 0.8421\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 3s 372us/step - loss: 0.3700 - acc: 0.8456\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 3s 369us/step - loss: 0.3669 - acc: 0.8476\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 3s 381us/step - loss: 0.3638 - acc: 0.8477\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 3s 369us/step - loss: 0.3616 - acc: 0.8511\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 3s 374us/step - loss: 0.3597 - acc: 0.8515\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 3s 370us/step - loss: 0.3569 - acc: 0.8520\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 3s 412us/step - loss: 0.3558 - acc: 0.8540\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 3s 401us/step - loss: 0.3542 - acc: 0.8550\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.3515 - acc: 0.8547\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.3498 - acc: 0.8552\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3489 - acc: 0.8564\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3491 - acc: 0.8580\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.3485 - acc: 0.8569\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 0.3478 - acc: 0.8586\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.3473 - acc: 0.8595\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.3454 - acc: 0.8587\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.3457 - acc: 0.8580\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3447 - acc: 0.8586\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.3448 - acc: 0.8599\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3437 - acc: 0.8590\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 0.3438 - acc: 0.8570\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3430 - acc: 0.8590\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3431 - acc: 0.8607\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.3433 - acc: 0.8594\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.3428 - acc: 0.8606\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 0.3420 - acc: 0.8595\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.3411 - acc: 0.8585\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 0.3414 - acc: 0.8601\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3412 - acc: 0.8595\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3395 - acc: 0.8614\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.3402 - acc: 0.8595\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.3407 - acc: 0.8581\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 0.3399 - acc: 0.8594\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3406 - acc: 0.8617\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 0.3399 - acc: 0.8599\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 0.3395 - acc: 0.8592\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 0.3390 - acc: 0.8611\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3390 - acc: 0.8626\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.3387 - acc: 0.8609 1s - los\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.3393 - acc: 0.8617\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.3390 - acc: 0.8619\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3380 - acc: 0.8605\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 0.3379 - acc: 0.8615\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.3381 - acc: 0.8625\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 0.3377 - acc: 0.8616\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 0.3370 - acc: 0.8612\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3378 - acc: 0.8622\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3378 - acc: 0.8622\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.3369 - acc: 0.8616\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.3375 - acc: 0.8602\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 0.3362 - acc: 0.8621\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.3364 - acc: 0.8642\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.3369 - acc: 0.8611\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3367 - acc: 0.8627\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 0.3363 - acc: 0.8620\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 310us/step - loss: 0.3369 - acc: 0.8616\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 5s 563us/step - loss: 0.3357 - acc: 0.8611\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 5s 580us/step - loss: 0.3354 - acc: 0.8626\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 5s 573us/step - loss: 0.3361 - acc: 0.8636\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 5s 575us/step - loss: 0.3357 - acc: 0.8636\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 5s 570us/step - loss: 0.3358 - acc: 0.8621\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 5s 575us/step - loss: 0.3363 - acc: 0.8616\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 3s 398us/step - loss: 0.3358 - acc: 0.8621\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 3s 395us/step - loss: 0.3352 - acc: 0.8642\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 3s 405us/step - loss: 0.3359 - acc: 0.8640\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 3s 396us/step - loss: 0.3354 - acc: 0.8622\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 4s 494us/step - loss: 0.3354 - acc: 0.8635\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 3s 404us/step - loss: 0.3358 - acc: 0.8639\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 3s 394us/step - loss: 0.3344 - acc: 0.8635\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 3s 409us/step - loss: 0.3347 - acc: 0.8641\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 3s 399us/step - loss: 0.3352 - acc: 0.8616\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 3s 397us/step - loss: 0.3347 - acc: 0.8641\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 3s 399us/step - loss: 0.3352 - acc: 0.8655\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 3s 397us/step - loss: 0.3344 - acc: 0.8636\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 3s 397us/step - loss: 0.3340 - acc: 0.8645\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 5s 642us/step - loss: 0.3330 - acc: 0.8655\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 4s 553us/step - loss: 0.3338 - acc: 0.8632\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 4s 516us/step - loss: 0.3332 - acc: 0.8621\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 4s 557us/step - loss: 0.3346 - acc: 0.8630\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 4s 511us/step - loss: 0.3345 - acc: 0.8644\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 4s 498us/step - loss: 0.3334 - acc: 0.8650\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 5s 667us/step - loss: 0.3346 - acc: 0.8631\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 4s 511us/step - loss: 0.3345 - acc: 0.8620\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 5s 567us/step - loss: 0.3338 - acc: 0.8649\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 4s 544us/step - loss: 0.3341 - acc: 0.8645\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 4s 504us/step - loss: 0.3353 - acc: 0.8602\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 4s 528us/step - loss: 0.3338 - acc: 0.8637\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 6s 718us/step - loss: 0.3339 - acc: 0.8614\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 4s 554us/step - loss: 0.3340 - acc: 0.8607\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 3s 394us/step - loss: 0.3338 - acc: 0.8630\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 5s 587us/step - loss: 0.3324 - acc: 0.8649\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 5s 659us/step - loss: 0.3341 - acc: 0.8635\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 5s 640us/step - loss: 0.3331 - acc: 0.8659\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 4s 543us/step - loss: 0.3331 - acc: 0.8621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29d26b3b5f8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the ANN to the training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ..., \n",
       "       [False],\n",
       "       [False],\n",
       "       [False]], dtype=bool)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_results = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/grfiv4/plot-a-confusion-matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAHCCAYAAACZsHHYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKQBFURRFMQH5N9bPPyIrCvZ0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29d1b23eef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm           = cm_results, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['Stay in the bank', 'Leave the Bank'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
