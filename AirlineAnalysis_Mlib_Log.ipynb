{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#  Download the required Dataset for analysis\n# ! wget http://stat-computing.org/dataexpo/2009/2007.csv.bz2\n# ! http://stat-computing.org/dataexpo/2009/2008.csv.bz2\n# ! wget https://github.com/jayyanar/MachineLearning_Workbook/blob/master/2007-ord-weather-data.csv --no-check-certificate\n# ! wget https://github.com/jayyanar/MachineLearning_Workbook/blob/master/2008-ord-weather-data.csv --no-check-certificate\n# ! bzip2 -d 2007.csv.bz2\n# ! bzip2 -d 2008.csv.bz2"
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "total 4415016\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users    569231 Mar 24  2013 joda-time-2.0.jar\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users 702878193 Aug 22  2014 2007.csv\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users 113753229 Dec  9  2014 2008.csv.bz2\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users 689413344 Dec  9  2014 2008.csv\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users    169922 Jul 31 04:40 2007-ord-weather-data.csv\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users    190795 Jul 31 04:41 2008-ord-weather-data.csv\r\ndrwx------ 2 s7ed-a18f3badb92bc2-a9f6794a31ec users      4096 Aug  2 03:08 joda\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users      1553 Aug  2 04:00 preprocess1.pig\r\n"
                }
            ], 
            "source": "! ls -lrt\n"
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "ParserError", 
                    "evalue": "Error tokenizing data. C error: Expected 1 fields in line 31, saw 3\n", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-26-331db8902aa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2007-ord-weather-data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1063\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 31, saw 3\n"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "df = pd.read_csv(\"2007-ord-weather-data.csv\")"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
                }
            ], 
            "source": "# For SQL-type queries (Spark)\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import *\n\n# For regression and other possible ML tools (Spark)\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.param import Param, Params\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.stat import Statistics\n\n\n# Important for managing features  (Spark)\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\n\n# For displaying and other related IPython tools...\nfrom IPython.display import display\nfrom IPython.html.widgets import interact\n\n# Typycal Python tools\nimport sys\nimport numpy as np\nimport pandas as pd\nimport time\nimport datetime\nimport matplotlib.pyplot as plt\nimport os.path"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# ### Creating a SQL Dataframe from RDD\n# \n# We now create a SQL DataFrame, this entity is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in Python, but with richer optimizations under the hood. We will utilize the recently created Spark RDD and use the Spark SQL context to create the desired data frame,\n\n# We first create function that will allow to parse a record of our RDD into the desired format. As a reference we take a look at features_names and feature_example we just created above\n\ndef parse(r):\n    try:\n        x=Row(Month=int(r[1]),\\\n          DayofMonth=int(r[2]),\\\n          DayOfWeek=int(r[3]),\\\n          CRSDepTime=int(r[5]),\\\n          DepDelay=int(float(r[15])),\\\n          Origin=r[16],\\\n          Distance=int(float(r[18]))) \n    except:\n        x=None  \n    return x\n\n# define hour function to obtain hour of day\ndef hour_ex(x): \n    h = int(str(int(x)).zfill(4)[:2])\n    return h\n# register as a UDF \nf = udf(hour_ex, IntegerType())\n"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def prepFlightDelay(infile, orgn):\n    textFile = sc.textFile(infile)\n    textFileRDD = textFile.map(lambda x: x.split(','))\n    header = textFileRDD.first()\n    textRDD = textFileRDD.filter(lambda r: r != header)\n    rowRDD = textRDD.map(lambda r: parse(r)).filter(lambda r:r != None)\n    airline_df = sqlContext.createDataFrame(rowRDD)\n    airline_df = airline_df.withColumn('hour', f(airline_df.CRSDepTime))\n    airline_df.registerTempTable(\"airlineDF\")\n    airline_df_ORD = airline_df.filter((col(\"Origin\") == orgn))\n    airline_df_ORD_15 = airline_df_ORD.withColumn('DepDelayed', (airline_df_ORD['DepDelay']>15).cast(DoubleType()))\n    return airline_df_ORD_15"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 5, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(CRSDepTime=1600, DayOfWeek=2, DayofMonth=8, DepDelay=71, Distance=719, Month=1, Origin='ORD', hour=16, DepDelayed=1.0),\n Row(CRSDepTime=1742, DayOfWeek=5, DayofMonth=25, DepDelay=31, Distance=316, Month=1, Origin='ORD', hour=17, DepDelayed=1.0),\n Row(CRSDepTime=1600, DayOfWeek=4, DayofMonth=10, DepDelay=1, Distance=719, Month=1, Origin='ORD', hour=16, DepDelayed=0.0),\n Row(CRSDepTime=650, DayOfWeek=5, DayofMonth=4, DepDelay=8, Distance=719, Month=1, Origin='ORD', hour=6, DepDelayed=0.0),\n Row(CRSDepTime=1205, DayOfWeek=4, DayofMonth=10, DepDelay=-5, Distance=925, Month=1, Origin='ORD', hour=12, DepDelayed=0.0)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "flight_2007_ORD = prepFlightDelay('2007.csv',\"ORD\")\nflight_2008_ORD = prepFlightDelay('2008.csv',\"ORD\")\nflight_2007_ORD.take(5)\nflight_2008_ORD.take(5)"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- CRSDepTime: long (nullable = true)\n |-- DayOfWeek: long (nullable = true)\n |-- DayofMonth: long (nullable = true)\n |-- DepDelay: long (nullable = true)\n |-- Distance: long (nullable = true)\n |-- Month: long (nullable = true)\n |-- Origin: string (nullable = true)\n |-- hour: integer (nullable = true)\n |-- DepDelayed: double (nullable = true)\n\n"
                }
            ], 
            "source": "flight_2007_ORD.printSchema()"
        }, 
        {
            "source": "## Create an Apache\u00ae Spark machine learning model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Prepare data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of training records: 359169\nNumber of testing records : 335440\n"
                }
            ], 
            "source": "#split_data = flight_2007_ORD.randomSplit([0.8, 0.20], 24)\ntrain_data = flight_2007_ORD\ntest_data = flight_2008_ORD\n\n\nprint (\"Number of training records: \" + str(train_data.count()))\nprint (\"Number of testing records : \" + str(test_data.count()))"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml import Pipeline, Model"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#In the following step, convert all the string fields to numeric ones by using the StringIndexer transformer.\nstringIndexer_label = StringIndexer(inputCol=\"DepDelayed\", outputCol=\"label\").fit(flight_2007_ORD)\nstringIndexer_org = StringIndexer(inputCol=\"Origin\", outputCol=\"Orgin_Airport\")"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#In the following step, create a feature vector by combining all features together.\n#[Row(CRSDepTime=1100, DayOfWeek=4, DayofMonth=25, DepDelay=-8, DepTime=1052, Dest='EWR', Distance=719, Month=1, Origin='ORD', hour=11, DepDelayed=False),\nvectorAssembler_features = VectorAssembler(inputCols=[\"CRSDepTime\",\"DayOfWeek\",\"DayofMonth\",\"DepDelay\",\"Distance\",\"Month\",\"Orgin_Airport\"], outputCol=\"features\")"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\",numTrees=10)\n#lr = LogisticRegressionWithSGD.train(scaledTrainData, numIterations=100)"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "DataFrame[CRSDepTime: bigint, DayOfWeek: bigint, DayofMonth: bigint, DepDelay: bigint, Distance: bigint, Month: bigint, Origin: string, hour: int, DepDelayed: double]\n"
                }
            ], 
            "source": "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=stringIndexer_label.labels)\nprint (flight_2007_ORD)"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+---------+----------+--------+--------+-----+------+----+----------+-----+-------------+--------------------+\n|CRSDepTime|DayOfWeek|DayofMonth|DepDelay|Distance|Month|Origin|hour|DepDelayed|label|Orgin_Airport|            features|\n+----------+---------+----------+--------+--------+-----+------+----+----------+-----+-------------+--------------------+\n|      1100|        4|        25|      -8|     719|    1|   ORD|  11|       0.0|  0.0|          0.0|[1100.0,4.0,25.0,...|\n|      1500|        7|        28|      41|     925|    1|   ORD|  15|       1.0|  1.0|          0.0|[1500.0,7.0,28.0,...|\n|      2000|        1|        29|      45|     316|    1|   ORD|  20|       1.0|  1.0|          0.0|[2000.0,1.0,29.0,...|\n|      1900|        3|        17|      -9|     719|    1|   ORD|  19|       0.0|  0.0|          0.0|[1900.0,3.0,17.0,...|\n|      1745|        5|        12|     180|     316|    1|   ORD|  17|       1.0|  1.0|          0.0|[1745.0,5.0,12.0,...|\n|       930|        5|        12|      29|     925|    1|   ORD|   9|       1.0|  1.0|          0.0|[930.0,5.0,12.0,2...|\n|      2000|        5|        26|      35|     316|    1|   ORD|  20|       1.0|  1.0|          0.0|[2000.0,5.0,26.0,...|\n|      1325|        2|         2|      -1|     316|    1|   ORD|  13|       0.0|  0.0|          0.0|[1325.0,2.0,2.0,-...|\n|      1600|        7|        28|       9|     719|    1|   ORD|  16|       0.0|  0.0|          0.0|[1600.0,7.0,28.0,...|\n|      1255|        2|         9|      -2|     719|    1|   ORD|  12|       0.0|  0.0|          0.0|[1255.0,2.0,9.0,-...|\n|      1545|        3|        17|      -3|     316|    1|   ORD|  15|       0.0|  0.0|          0.0|[1545.0,3.0,17.0,...|\n|      1255|        3|        10|       3|     719|    1|   ORD|  12|       0.0|  0.0|          0.0|[1255.0,3.0,10.0,...|\n|      1510|        6|        20|      61|     719|    1|   ORD|  15|       1.0|  1.0|          0.0|[1510.0,6.0,20.0,...|\n|       935|        5|        26|     -11|     316|    1|   ORD|   9|       0.0|  0.0|          0.0|[935.0,5.0,26.0,-...|\n|      1545|        1|         8|     -12|     316|    1|   ORD|  15|       0.0|  0.0|          0.0|[1545.0,1.0,8.0,-...|\n|      1900|        4|        11|     -10|     719|    1|   ORD|  19|       0.0|  0.0|          0.0|[1900.0,4.0,11.0,...|\n|      1330|        3|        17|      -8|     316|    1|   ORD|  13|       0.0|  0.0|          0.0|[1330.0,3.0,17.0,...|\n|       935|        1|        29|      61|     316|    1|   ORD|   9|       1.0|  1.0|          0.0|[935.0,1.0,29.0,6...|\n|       930|        3|        10|      45|     925|    1|   ORD|   9|       1.0|  1.0|          0.0|[930.0,3.0,10.0,4...|\n|      1100|        7|        28|       0|     719|    1|   ORD|  11|       0.0|  0.0|          0.0|[1100.0,7.0,28.0,...|\n+----------+---------+----------+--------+--------+-----+------+----+----------+-----+-------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "transform_df_pipeline = Pipeline(stages=[stringIndexer_label, stringIndexer_org, vectorAssembler_features])\ntransformed_df = transform_df_pipeline.fit(flight_2007_ORD).transform(flight_2007_ORD)\ntransformed_df.show()"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_lr = lr.fit(transformed_df)"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Coefficients: (7,[3],[0.007051085553957692])\nIntercept: -0.995146858518747\n"
                }
            ], 
            "source": "# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(model_lr.coefficients))\nprint(\"Intercept: \" + str(model_lr.intercept))"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "objectiveHistory:\n0.6070093019198632\n0.6030360245813051\n0.5996266965654328\n0.5970178159072881\n0.5963395696875052\n0.5956965660661498\n0.595472771841593\n0.5952198597394407\n0.595073050772862\n0.5949848693798332\n0.5949714990511628\n+---+--------------------+\n|FPR|                 TPR|\n+---+--------------------+\n|0.0|                 0.0|\n|0.0|4.710892525697918...|\n|0.0|9.421785051395837E-5|\n|0.0|1.507485608223334E-4|\n|0.0|2.072792711307084...|\n|0.0|2.449664113362918E-4|\n|0.0| 2.92075336593271E-4|\n|0.0|3.391842618502501...|\n|0.0|3.862931871072293...|\n|0.0|4.428238974156043...|\n|0.0|4.805110376211877E-4|\n|0.0|5.370417479295627E-4|\n|0.0|5.935724582379377E-4|\n|0.0|6.501031685463128E-4|\n|0.0|7.160556639060836E-4|\n|0.0|7.725863742144587E-4|\n|0.0|8.291170845228337E-4|\n|0.0|9.233349350367921E-4|\n|0.0|0.001092927065961...|\n|0.0|0.001158879561321688|\n+---+--------------------+\nonly showing top 20 rows\n\nareaUnderROC: 0.9999999999999999\n"
                }, 
                {
                    "execution_count": 20, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "LogisticRegression_4cd8bfd76441143fd31b"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# Extract the summary from the returned LogisticRegressionModel instance trained\n# in the earlier example\ntrainingSummary = model_lr.summary\n\n# Obtain the objective per iteration\nobjectiveHistory = trainingSummary.objectiveHistory\nprint(\"objectiveHistory:\")\nfor objective in objectiveHistory:\n    print(objective)\n\n# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\ntrainingSummary.roc.show()\nprint(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n\n# Set the model threshold to maximize F-Measure\nfMeasure = trainingSummary.fMeasureByThreshold\nmaxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\nbestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n    .select('threshold').head()['threshold']\nlr.setThreshold(bestThreshold)"
        }, 
        {
            "source": "## Random Forest", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipeline_rf = Pipeline(stages=[stringIndexer_label, stringIndexer_org, vectorAssembler_features, rf, labelConverter])"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_rf = pipeline_rf.fit(train_data)"
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Accuracy for Random Forest = 0.992121\nTest Error for Random Forest = 0.0078792\n"
                }
            ], 
            "source": "predictions = model_rf.transform(test_data)\nevaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluatorRF.evaluate(predictions)\nprint(\"Accuracy for Random Forest = %g\" % accuracy)\nprint(\"Test Error for Random Forest = %g\" % (1.0 - accuracy))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#\n# Python UDFs for our PIG script\n#\nfrom datetime import date\n\n# this array defines the dates of holiday in 2007 and 2008\nholidays = [\n        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25), \\\n        date(2008, 1, 1), date(2008, 1, 21), date(2008, 2, 18), date(2008, 5, 22), date(2008, 5, 26), date(2008, 7, 4), \\\n        date(2008, 9, 1), date(2008, 10, 13), date(2008, 11, 11), date(2008, 11, 27), date(2008, 12, 25) \\\n     ]\n# get number of days from nearest holiday\ndef days_from_nearest_holiday(year, month, day):\n    d = date(year, month, day)\n    x = [(abs(d-h)).days for h in holidays]\n    return min(x)\ndef to_date(year, month, day):\n    td = date(year, month, day)\n    return td"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "! ls -lrta"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}