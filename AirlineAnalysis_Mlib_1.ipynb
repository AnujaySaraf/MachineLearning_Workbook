{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#  Download the required Dataset for analysis\n# ! wget http://stat-computing.org/dataexpo/2009/2007.csv.bz2\n# ! http://stat-computing.org/dataexpo/2009/2008.csv.bz2\n# ! wget https://github.com/jayyanar/MachineLearning_Workbook/blob/master/2007-ord-weather-data.csv --no-check-certificate\n# ! wget https://github.com/jayyanar/MachineLearning_Workbook/blob/master/2008-ord-weather-data.csv --no-check-certificate\n# ! bzip2 -d 2007.csv.bz2\n# ! bzip2 -d 2008.csv.bz2"
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "total 4415016\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users    569231 Mar 24  2013 joda-time-2.0.jar\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users 702878193 Aug 22  2014 2007.csv\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users 113753229 Dec  9  2014 2008.csv.bz2\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users 689413344 Dec  9  2014 2008.csv\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users    169922 Jul 31 04:40 2007-ord-weather-data.csv\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users    190795 Jul 31 04:41 2008-ord-weather-data.csv\r\ndrwx------ 2 s7ed-a18f3badb92bc2-a9f6794a31ec users      4096 Aug  2 03:08 joda\r\n-rw------- 1 s7ed-a18f3badb92bc2-a9f6794a31ec users      1553 Aug  2 04:00 preprocess1.pig\r\n"
                }
            ], 
            "source": "! ls -lrt\n"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
                }
            ], 
            "source": "# For SQL-type queries (Spark)\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import *\n\n# For regression and other possible ML tools (Spark)\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.param import Param, Params\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.stat import Statistics\n\n\n# Important for managing features  (Spark)\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\n\n# For displaying and other related IPython tools...\nfrom IPython.display import display\nfrom IPython.html.widgets import interact\n\n# Typycal Python tools\nimport sys\nimport numpy as np\nimport pandas as pd\nimport time\nimport datetime\nimport matplotlib.pyplot as plt\nimport os.path"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# ### Creating a SQL Dataframe from RDD\n# \n# We now create a SQL DataFrame, this entity is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in Python, but with richer optimizations under the hood. We will utilize the recently created Spark RDD and use the Spark SQL context to create the desired data frame,\n\n# We first create function that will allow to parse a record of our RDD into the desired format. As a reference we take a look at features_names and feature_example we just created above\n\ndef parse(r):\n    try:\n        x=Row(Month=int(r[1]),\\\n          DayofMonth=int(r[2]),\\\n          DayOfWeek=int(r[3]),\\\n          DepTime=int(float(r[4])), \\\n          CRSDepTime=int(r[5]),\\\n          DepDelay=int(float(r[15])),\\\n          Origin=r[16],\\\n          Dest=r[17], \\\n          Distance=int(float(r[18]))) \n    except:\n        x=None  \n    return x\n\n# define hour function to obtain hour of day\ndef hour_ex(x): \n    h = int(str(int(x)).zfill(4)[:2])\n    return h\n# register as a UDF \nf = udf(hour_ex, IntegerType())\n"
        }, 
        {
            "execution_count": 74, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def prepFlightDelay(infile, orgn):\n    textFile = sc.textFile(infile)\n    textFileRDD = textFile.map(lambda x: x.split(','))\n    header = textFileRDD.first()\n    textRDD = textFileRDD.filter(lambda r: r != header)\n    rowRDD = textRDD.map(lambda r: parse(r)).filter(lambda r:r != None)\n    airline_df = sqlContext.createDataFrame(rowRDD)\n    airline_df = airline_df.withColumn('hour', f(airline_df.CRSDepTime))\n    airline_df.registerTempTable(\"airlineDF\")\n    airline_df_ORD = airline_df.filter((col(\"Origin\") == orgn))\n    airline_df_ORD_15 = airline_df_ORD.withColumn('DepDelayed', airline_df_ORD['DepDelay']>15)\n    return airline_df_ORD_15"
        }, 
        {
            "execution_count": 102, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 102, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(CRSDepTime=1100, DayOfWeek=4, DayofMonth=25, DepDelay=-8, DepTime=1052, Dest='EWR', Distance=719, Month=1, Origin='ORD', hour=11, DepDelayed=False),\n Row(CRSDepTime=1500, DayOfWeek=7, DayofMonth=28, DepDelay=41, DepTime=1541, Dest='IAH', Distance=925, Month=1, Origin='ORD', hour=15, DepDelayed=True),\n Row(CRSDepTime=2000, DayOfWeek=1, DayofMonth=29, DepDelay=45, DepTime=2045, Dest='CLE', Distance=316, Month=1, Origin='ORD', hour=20, DepDelayed=True),\n Row(CRSDepTime=1900, DayOfWeek=3, DayofMonth=17, DepDelay=-9, DepTime=1851, Dest='EWR', Distance=719, Month=1, Origin='ORD', hour=19, DepDelayed=False),\n Row(CRSDepTime=1745, DayOfWeek=5, DayofMonth=12, DepDelay=180, DepTime=2045, Dest='CLE', Distance=316, Month=1, Origin='ORD', hour=17, DepDelayed=True)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "flight_2007_ORD = prepFlightDelay('2007.csv',\"ORD\")\nflight_2007_ORD.take(5)"
        }, 
        {
            "execution_count": 112, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "SyntaxError", 
                    "evalue": "invalid syntax (<ipython-input-112-179fd1989cd2>, line 11)", 
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-112-179fd1989cd2>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    flight_2007_ORD = flight_2007_ORD.withColumn(\"DepDelayed\",dflight_2007_ORD[\"DepDelayed\"].cast(\"string)).drop(flight_2007_ORD[\"DepDelayed\"])\u001b[0m\n\u001b[0m                                                                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "\nprint (\"Actual Schema of the df\")\nflight_2007_ORD.printSchema()\n\nfor a_dftype in flight_2007_ORD.dtypes:\n    #col_name = a_dftype[0]\n    col_type = a_dftype[:1]\n#     print df.select(col_name).collect()[0][0]\n    \n    if col_type == 'boolean' and (flight_2007_ORD.select(\"DepDelayed\").distinct().collect()[0][0] =='False' or flight_2007_ORD.select(\"DepDelayed\").distinct().collect()[0][0] =='True'):\n        flight_2007_ORD = flight_2007_ORD.withColumn(\"DepDelayed\",dflight_2007_ORD[\"DepDelayed\"].cast(\"string).drop(flight_2007_ORD[\"DepDelayed\"])\n\nprint (\"Modified Schema of the df\")\nflight_2007_ORD.printSchema()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#df = sc.parallelize([(1, 'Y','F',\"Giri\",'Y'), (2, 'N','V',\"Databricks\",'N'),(3,'Y','B',\"SparkEdge\",'Y'),(4,'N','X',\"Spark\",'N')]).toDF([\"id\", \"flag1\",\"flag2\",\"name\",\"flag3\"])\nprint (\"Show Dataframe\")\n#df.show()\nprint (\"Actual Schema of the df\")\ndf.printSchema()\n\nfor a_dftype in df.dtypes:\n    col_name = a_dftype[0]\n    col_type = a_dftype[1]\n#     print df.select(col_name).collect()[0][0]\n    \n    if col_type=='string' and (df.select(col_name).distinct().collect()[0][0] =='N' or df.select(col_name).distinct().collect()[0][0] =='Y'):\n        df = df.withColumn(col_name,df[col_name].cast(\"boolean\")).drop(df[col_name])\n    else:\n        df = df.withColumn(col_name,df[col_name]).drop(df[col_name])\nprint (\"df with True/False Value after Data Type changes\")\ndf.show()\nprint (\"Modified Schema of the df\")\ndf.printSchema()"
        }, 
        {
            "execution_count": 103, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 103, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(CRSDepTime=1100, DayOfWeek=4, DayofMonth=25, DepDelay=-8, DepTime=1052, Dest='EWR', Distance=719, Month=1, Origin='ORD', hour=11, DepDelayed=None),\n Row(CRSDepTime=1500, DayOfWeek=7, DayofMonth=28, DepDelay=41, DepTime=1541, Dest='IAH', Distance=925, Month=1, Origin='ORD', hour=15, DepDelayed=None),\n Row(CRSDepTime=2000, DayOfWeek=1, DayofMonth=29, DepDelay=45, DepTime=2045, Dest='CLE', Distance=316, Month=1, Origin='ORD', hour=20, DepDelayed=None),\n Row(CRSDepTime=1900, DayOfWeek=3, DayofMonth=17, DepDelay=-9, DepTime=1851, Dest='EWR', Distance=719, Month=1, Origin='ORD', hour=19, DepDelayed=None),\n Row(CRSDepTime=1745, DayOfWeek=5, DayofMonth=12, DepDelay=180, DepTime=2045, Dest='CLE', Distance=316, Month=1, Origin='ORD', hour=17, DepDelayed=None)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "#if flight_2007_ORD.select(col(\"DepDelayed\")) == \"False\" \n#textFileRDD = textFile.map(lambda x: x.split(','))\n#new_column_3 = coalesce((col(\"fruit1\") == col(\"fruit2\")).cast(\"int\"), lit(3))\n#flight_2007_ORD.coalesce((col(\"DepDelayed\" == \"True\"), lit(\"Y\"))).show()\n#repl1 = flight_2007_ORD.withColumn(\"DepDelayed\",lit(\"Y\"))\n\n#targetDf = df.withColumn(\"timestamp1\", when(df[\"session\"] == 0, 999).otherwise(df[\"timestamp1\"]))\ntargetDf = flight_2007_ORD.withColumn(\"DepDelayed\", when(flight_2007_ORD[\"DepDelayed\"] == \"True\", \"Y\"))\ntargetDf.take(5)"
        }, 
        {
            "execution_count": 81, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "SyntaxError", 
                    "evalue": "invalid syntax (<ipython-input-81-08fc292432bb>, line 1)", 
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-81-08fc292432bb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    flight_2007_ORD.select(\"flight_2007_ORD.DepDelayed\".as (\"F\"))\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "flight_2007_ORD.select(\"flight_2007_ORD.DepDelayed\".as (\"F\"))"
        }, 
        {
            "source": "## Create an Apache\u00ae Spark machine learning model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Prepare data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 39, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of training records: 287074\nNumber of testing records : 72095\n"
                }
            ], 
            "source": "split_data = flight_2007_ORD.randomSplit([0.8, 0.20], 24)\ntrain_data = split_data[0]\ntest_data = split_data[1]\n\n\nprint (\"Number of training records: \" + str(train_data.count()))\nprint (\"Number of testing records : \" + str(test_data.count()))"
        }, 
        {
            "execution_count": 40, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler, Binarizer\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model"
        }, 
        {
            "execution_count": 59, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#In the following step, convert all the string fields to numeric ones by using the StringIndexer transformer.\n#stringIndexer_label = Binarizer(inputCol=\"DepDelayed\", outputCol=\"Delayed\")\nstringIndexer_dest = StringIndexer(inputCol=\"Dest\", outputCol=\"Destination\").fit(flight_2007_ORD)\nstringIndexer_org = StringIndexer(inputCol=\"Origin\", outputCol=\"Orgin_Airport\")"
        }, 
        {
            "execution_count": 60, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#In the following step, create a feature vector by combining all features together.\n#[Row(CRSDepTime=1100, DayOfWeek=4, DayofMonth=25, DepDelay=-8, DepTime=1052, Dest='EWR', Distance=719, Month=1, Origin='ORD', hour=11, DepDelayed=False),\nvectorAssembler_features = VectorAssembler(inputCols=[\"CRSDepTime\",\"DayOfWeek\",\"DayofMonth\",\"DepDelay\",\"DepTime\",\"Destination\",\"Distance\",\"Month\",\"Orgin_Airport\"], outputCol=\"features\")"
        }, 
        {
            "execution_count": 61, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "rf = RandomForestClassifier(labelCol=\"DepDelayed\", featuresCol=\"features\")"
        }, 
        {
            "execution_count": 62, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "DataFrame[CRSDepTime: bigint, DayOfWeek: bigint, DayofMonth: bigint, DepDelay: bigint, DepTime: bigint, Dest: string, Distance: bigint, Month: bigint, Origin: string, hour: int, DepDelayed: boolean]\n"
                }
            ], 
            "source": "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=stringIndexer_dest.labels)\nprint (flight_2007_ORD)"
        }, 
        {
            "execution_count": 65, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------+---------+----------+--------+-------+----+--------+-----+------+----+----------+-----------+-------------+--------------------+\n|CRSDepTime|DayOfWeek|DayofMonth|DepDelay|DepTime|Dest|Distance|Month|Origin|hour|DepDelayed|Destination|Orgin_Airport|            features|\n+----------+---------+----------+--------+-------+----+--------+-----+------+----+----------+-----------+-------------+--------------------+\n|      1100|        4|        25|      -8|   1052| EWR|     719|    1|   ORD|  11|     false|        6.0|          0.0|[1100.0,4.0,25.0,...|\n|      1500|        7|        28|      41|   1541| IAH|     925|    1|   ORD|  15|      true|       16.0|          0.0|[1500.0,7.0,28.0,...|\n|      2000|        1|        29|      45|   2045| CLE|     316|    1|   ORD|  20|      true|       14.0|          0.0|[2000.0,1.0,29.0,...|\n|      1900|        3|        17|      -9|   1851| EWR|     719|    1|   ORD|  19|     false|        6.0|          0.0|[1900.0,3.0,17.0,...|\n|      1745|        5|        12|     180|   2045| CLE|     316|    1|   ORD|  17|      true|       14.0|          0.0|[1745.0,5.0,12.0,...|\n|       930|        5|        12|      29|    959| IAH|     925|    1|   ORD|   9|      true|       16.0|          0.0|[930.0,5.0,12.0,2...|\n|      2000|        5|        26|      35|   2035| CLE|     316|    1|   ORD|  20|      true|       14.0|          0.0|[2000.0,5.0,26.0,...|\n|      1325|        2|         2|      -1|   1324| CLE|     316|    1|   ORD|  13|     false|       14.0|          0.0|[1325.0,2.0,2.0,-...|\n|      1600|        7|        28|       9|   1609| EWR|     719|    1|   ORD|  16|     false|        6.0|          0.0|[1600.0,7.0,28.0,...|\n|      1255|        2|         9|      -2|   1253| EWR|     719|    1|   ORD|  12|     false|        6.0|          0.0|[1255.0,2.0,9.0,-...|\n|      1545|        3|        17|      -3|   1542| CLE|     316|    1|   ORD|  15|     false|       14.0|          0.0|[1545.0,3.0,17.0,...|\n|      1255|        3|        10|       3|   1258| EWR|     719|    1|   ORD|  12|     false|        6.0|          0.0|[1255.0,3.0,10.0,...|\n|      1510|        6|        20|      61|   1611| EWR|     719|    1|   ORD|  15|      true|        6.0|          0.0|[1510.0,6.0,20.0,...|\n|       935|        5|        26|     -11|    924| CLE|     316|    1|   ORD|   9|     false|       14.0|          0.0|[935.0,5.0,26.0,-...|\n|      1545|        1|         8|     -12|   1533| CLE|     316|    1|   ORD|  15|     false|       14.0|          0.0|[1545.0,1.0,8.0,-...|\n|      1900|        4|        11|     -10|   1850| EWR|     719|    1|   ORD|  19|     false|        6.0|          0.0|[1900.0,4.0,11.0,...|\n|      1330|        3|        17|      -8|   1322| CLE|     316|    1|   ORD|  13|     false|       14.0|          0.0|[1330.0,3.0,17.0,...|\n|       935|        1|        29|      61|   1036| CLE|     316|    1|   ORD|   9|      true|       14.0|          0.0|[935.0,1.0,29.0,6...|\n|       930|        3|        10|      45|   1015| IAH|     925|    1|   ORD|   9|      true|       16.0|          0.0|[930.0,3.0,10.0,4...|\n|      1100|        7|        28|       0|   1100| EWR|     719|    1|   ORD|  11|     false|        6.0|          0.0|[1100.0,7.0,28.0,...|\n+----------+---------+----------+--------+-------+----+--------+-----+------+----+----------+-----------+-------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "transform_df_pipeline = Pipeline(stages=[stringIndexer_dest, stringIndexer_org, vectorAssembler_features])\ntransformed_df = transform_df_pipeline.fit(flight_2007_ORD).transform(flight_2007_ORD)\ntransformed_df.show()"
        }, 
        {
            "execution_count": 66, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#pipeline_rf = Pipeline(stages=[stringIndexer_label, stringIndexer_sex, stringIndexer_famhist, stringIndexer_smoker, vectorAssembler_features, rf, labelConverter])\n\npipeline_rf = Pipeline(stages=[stringIndexer_dest, stringIndexer_org, vectorAssembler_features, rf, labelConverter])\n"
        }, 
        {
            "execution_count": 67, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "IllegalArgumentException", 
                    "evalue": "'requirement failed: Column DepDelayed must be of type NumericType but was actually of type BooleanType.'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o500.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column DepDelayed must be of type NumericType but was actually of type BooleanType.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:73)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:53)\n\tat org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:58)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:42)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:122)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:90)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:72)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:90)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:811)\n", 
                        "\nDuring handling of the above exception, another exception occurred:\n", 
                        "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-67-c2a388530c85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column DepDelayed must be of type NumericType but was actually of type BooleanType.'"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "model_rf = pipeline_rf.fit(train_data)"
        }, 
        {
            "execution_count": 68, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "NameError", 
                    "evalue": "name 'model_rf' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-68-2eebad493250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mevaluatorRF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMulticlassClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluatorRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy = %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Error = %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mNameError\u001b[0m: name 'model_rf' is not defined"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "predictions = model_rf.transform(test_data)\nevaluatorRF = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluatorRF.evaluate(predictions)\nprint(\"Accuracy = %g\" % accuracy)\nprint(\"Test Error = %g\" % (1.0 - accuracy))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#\n# Python UDFs for our PIG script\n#\nfrom datetime import date\n\n# this array defines the dates of holiday in 2007 and 2008\nholidays = [\n        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25), \\\n        date(2008, 1, 1), date(2008, 1, 21), date(2008, 2, 18), date(2008, 5, 22), date(2008, 5, 26), date(2008, 7, 4), \\\n        date(2008, 9, 1), date(2008, 10, 13), date(2008, 11, 11), date(2008, 11, 27), date(2008, 12, 25) \\\n     ]\n# get number of days from nearest holiday\ndef days_from_nearest_holiday(year, month, day):\n    d = date(year, month, day)\n    x = [(abs(d-h)).days for h in holidays]\n    return min(x)\ndef to_date(year, month, day):\n    td = date(year, month, day)\n    return td"
        }, 
        {
            "execution_count": 69, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "total 4415016\r\n-rw-------  1 s7ed-a18f3badb92bc2-a9f6794a31ec users    569231 Mar 24  2013 joda-time-2.0.jar\r\n-rw-------  1 s7ed-a18f3badb92bc2-a9f6794a31ec users 702878193 Aug 22  2014 2007.csv\r\n-rw-------  1 s7ed-a18f3badb92bc2-a9f6794a31ec users 113753229 Dec  9  2014 2008.csv.bz2\r\n-rw-------  1 s7ed-a18f3badb92bc2-a9f6794a31ec users 689413344 Dec  9  2014 2008.csv\r\ndrwx------ 11 s7ed-a18f3badb92bc2-a9f6794a31ec users      4096 Jul 20 06:55 ..\r\n-rw-------  1 s7ed-a18f3badb92bc2-a9f6794a31ec users    169922 Jul 31 04:40 2007-ord-weather-data.csv\r\n-rw-------  1 s7ed-a18f3badb92bc2-a9f6794a31ec users    190795 Jul 31 04:41 2008-ord-weather-data.csv\r\ndrwx------  2 s7ed-a18f3badb92bc2-a9f6794a31ec users      4096 Aug  2 03:08 joda\r\ndrwx------  3 s7ed-a18f3badb92bc2-a9f6794a31ec users      4096 Aug  2 04:00 .\r\n-rw-------  1 s7ed-a18f3badb92bc2-a9f6794a31ec users      1553 Aug  2 04:00 preprocess1.pig\r\n"
                }
            ], 
            "source": "! ls -lrta"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}