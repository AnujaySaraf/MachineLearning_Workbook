{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#  Download the required Dataset for analysis\n# ! wget http://stat-computing.org/dataexpo/2009/2007.csv.bz2\n# ! http://stat-computing.org/dataexpo/2009/2008.csv.bz2\n# ! wget https://github.com/jayyanar/MachineLearning_Workbook/blob/master/2007-ord-weather-data.csv --no-check-certificate\n# ! wget https://github.com/jayyanar/MachineLearning_Workbook/blob/master/2008-ord-weather-data.csv --no-check-certificate\n# ! bzip2 -d 2007.csv.bz2\n# ! bzip2 -d 2008.csv.bz2"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "! ls -lrt\n"
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
                }
            ], 
            "source": "# For SQL-type queries (Spark)\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import *\n\n# For regression and other possible ML tools (Spark)\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.param import Param, Params\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.stat import Statistics\n\n\n# Important for managing features  (Spark)\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\n\n# For displaying and other related IPython tools...\nfrom IPython.display import display\nfrom IPython.html.widgets import interact\n\n# Typycal Python tools\nimport sys\nimport numpy as np\nimport pandas as pd\nimport time\nimport datetime\nimport matplotlib.pyplot as plt\nimport os.path"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# To show plots inline\nget_ipython().magic(u'matplotlib inline')"
        }, 
        {
            "source": "# function to read HDFS file into dataframe using PyDoop\nimport pydoop.hdfs as hdfs\ndef read_csv_from_hdfs(path, cols, col_types=None):\n  files = hdfs.ls(path);\n  pieces = []\n  for f in files:\n    fhandle = hdfs.open(f)\n    pieces.append(pd.read_csv(fhandle, names=cols, dtype=col_types))\n    fhandle.close()\n  return pd.concat(pieces, ignore_index=True)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# read 2007 year file\ncols = ['year', 'month', 'day', 'dow', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime', 'Carrier', 'FlightNum', \n        'TailNum', 'ActualElapsedTime', 'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', \n        'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'CancellationCode', 'Diverted', 'CarrierDelay', \n        'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay'];\nflt_2007 = read_csv_from_hdfs('airline/delay/2007.csv', cols)\n\nflt_2007.shape", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "textFile = sc.textFile('2007.csv')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "textFileRDD = textFile.map(lambda x: x.split(','))\nheader = textFileRDD.first()\ntextRDD = textFileRDD.filter(lambda r: r != header)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "num_records = textFileRDD.count()\nprint ('Number of records ' , num_records)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "aux_ = textFileRDD.take(2)\nfeature_names = aux_[0]\nfeature_example = aux_[1]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print ('Feature Names ' , feature_names)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print ('Feature Example ' , feature_example)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (\"Number of features = \" , len(feature_example))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (type(feature_example))"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# ### Creating a SQL Dataframe from RDD\n# \n# We now create a SQL DataFrame, this entity is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in Python, but with richer optimizations under the hood. We will utilize the recently created Spark RDD and use the Spark SQL context to create the desired data frame,\n\n# We first create function that will allow to parse a record of our RDD into the desired format. As a reference we take a look at features_names and feature_example we just created above\n\ndef parse(r):\n    try:\n        x=Row(Month=int(r[1]),\\\n          DayofMonth=int(r[2]),\\\n          DayOfWeek=int(r[3]),\\\n          DepTime=int(float(r[4])), \\\n          CRSDepTime=int(r[5]),\\\n          DepDelay=int(float(r[15])),\\\n          Origin=r[16],\\\n          Dest=r[17], \\\n          Distance=int(float(r[18]))) \n    except:\n        x=None  \n    return x\n\n# define hour function to obtain hour of day\ndef hour_ex(x): \n    h = int(str(int(x)).zfill(4)[:2])\n    return h\n# register as a UDF \nf = udf(hour_ex, IntegerType())\n"
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def prepFlightDelay(infile, orgn):\n    textFile = sc.textFile(infile)\n    textFileRDD = textFile.map(lambda x: x.split(','))\n    header = textFileRDD.first()\n    textRDD = textFileRDD.filter(lambda r: r != header)\n    rowRDD = textRDD.map(lambda r: parse(r)).filter(lambda r:r != None)\n    airline_df = sqlContext.createDataFrame(rowRDD)\n    airline_df = airline_df.withColumn('hour', f(airline_df.CRSDepTime))\n    airline_df.registerTempTable(\"airlineDF\")\n    airline_df_ORD = airline_df.filter((col(\"Origin\") == orgn))\n    airline_df_ORD_15 = airline_df_ORD.withColumn('DepDelayed', airline_df_ORD['DepDelay']>15)\n    return airline_df_ORD_15"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "flight_2007_ORD = prepFlightDelay('2007.csv',\"ORD\")"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 30, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[Row(CRSDepTime=1100, DayOfWeek=4, DayofMonth=25, DepDelay=-8, DepTime=1052, Dest='EWR', Distance=719, Month=1, Origin='ORD', Year=2007, hour=11, DepDelayed=False),\n Row(CRSDepTime=1500, DayOfWeek=7, DayofMonth=28, DepDelay=41, DepTime=1541, Dest='IAH', Distance=925, Month=1, Origin='ORD', Year=2007, hour=15, DepDelayed=True),\n Row(CRSDepTime=2000, DayOfWeek=1, DayofMonth=29, DepDelay=45, DepTime=2045, Dest='CLE', Distance=316, Month=1, Origin='ORD', Year=2007, hour=20, DepDelayed=True),\n Row(CRSDepTime=1900, DayOfWeek=3, DayofMonth=17, DepDelay=-9, DepTime=1851, Dest='EWR', Distance=719, Month=1, Origin='ORD', Year=2007, hour=19, DepDelayed=False),\n Row(CRSDepTime=1745, DayOfWeek=5, DayofMonth=12, DepDelay=180, DepTime=2045, Dest='CLE', Distance=316, Month=1, Origin='ORD', Year=2007, hour=17, DepDelayed=True)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "flight_2007_ORD.take(5)"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#\n# Python UDFs for our PIG script\n#\nfrom datetime import date\n\n# this array defines the dates of holiday in 2007 and 2008\nholidays = [\n        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25), \\\n        date(2008, 1, 1), date(2008, 1, 21), date(2008, 2, 18), date(2008, 5, 22), date(2008, 5, 26), date(2008, 7, 4), \\\n        date(2008, 9, 1), date(2008, 10, 13), date(2008, 11, 11), date(2008, 11, 27), date(2008, 12, 25) \\\n     ]\n# get number of days from nearest holiday\ndef days_from_nearest_holiday(year, month, day):\n    d = date(year, month, day)\n    x = [(abs(d-h)).days for h in holidays]\n    return min(x)\ndef to_date(year, month, day):\n    td = date(year, month, day)\n    return td"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# We add a new column to our data frame, **DepDelayed**, a binary variable:\n# - **True**, for flights that have > 15 minutes of delay\n# - **False**, for flights that have <= 15 minutes of delay\n# \n# We will later use **Depdelayed** as the target/label column in the classification process.\n\n\n#airline_df = airline_df.withColumn('DepDelayed', airline_df['DepDelay']>15)\n#airline_df = airline_df.withColumn('DepDelayed',airline_df['Origin']==\"ORD\")\n#airline_df_ORD = airline_df.withColumn('DepDelayed', airline_df.filter((col(\"Origin\") == \"ORD\") | (col(\"DepDelay\")>15)))\nairline_df_ORD = airline_df.filter((col(\"Origin\") == \"ORD\"))\nairline_df_ORD_15 = airline_df_ORD.withColumn('DepDelayed', airline_df_ORD['DepDelay']>15)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "airline_df_ORD_15.take(10)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print (\"Total flights Origin at ORD Chicago Airport: \" + str(airline_df_ORD_15.count()))\nprint (airline_df_ORD_15.groupby('DepDelayed').count().show())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#airline_df_ORD_15.take(10)\nairline_df_ORD_15.filter(airline_df_ORD_15.DepDelayed == True).groupby(airline_df_ORD_15.Month).count().sort(asc(\"Month\")).show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_pandas = airline_df_ORD_15.toPandas()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_pandas.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Compute average number of delayed flights by hour\ndf_pandas['hour'] = df_pandas['CRSDepTime'].map(lambda x: int(str(int(x)).zfill(4)[:2]))\ngrouped = df_pandas[['DepDelayed', 'hour']].groupby('hour').mean()\n\n# plot average delays by hour of day\ngrouped.plot(kind='bar')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Compute average number of delayed flights per carrier\ngrouped1 = df_pandas[['DepDelayed', 'UniqueCarrier']].groupby('UniqueCarrier').filter(lambda x: len(x)>10)\ngrouped2 = grouped1.groupby('UniqueCarrier').mean()\ncarrier = grouped2.sort_values(['DepDelayed'], ascending=False)\n\n# display top 20 destination carriers by delay (from ORD)\ncarrier[:20].plot(kind='bar')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_pandas.info()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "! wget http://www.java2s.com/Code/JarDownload/joda/joda-time-2.0.jar.zip"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "! mkdir joda"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "! mv joda-time-2.0.jar.zip joda"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "! ls -rtl "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "! cd joda"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "! ls -rtla"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "! unzip  joda/joda-time-2.0.jar.zip"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%configure"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import org.apache.spark.rdd._\nimport scala.collection.JavaConverters._\nimport au.com.bytecode.opencsv.CSVReader\n\nimport java.io._\nimport org.joda.time._\nimport org.joda.time.format._\nimport org.joda.time.format.DateTimeFormat\nimport org.joda.time.DateTime\nimport org.joda.time.Days\n\n\ncase class DelayRec(year: String,\n                    month: String,\n                    dayOfMonth: String,\n                    dayOfWeek: String,\n                    crsDepTime: String,\n                    depDelay: String,\n                    origin: String,\n                    distance: String,\n                    cancelled: String) {\n\n    val holidays = List(\"01/01/2007\", \"01/15/2007\", \"02/19/2007\", \"05/28/2007\", \"06/07/2007\", \"07/04/2007\",\n      \"09/03/2007\", \"10/08/2007\" ,\"11/11/2007\", \"11/22/2007\", \"12/25/2007\",\n      \"01/01/2008\", \"01/21/2008\", \"02/18/2008\", \"05/22/2008\", \"05/26/2008\", \"07/04/2008\",\n      \"09/01/2008\", \"10/13/2008\" ,\"11/11/2008\", \"11/27/2008\", \"12/25/2008\")\n\n    def gen_features: (String, Array[Double]) = {\n      val values = Array(\n        depDelay.toDouble,\n        month.toDouble,\n        dayOfMonth.toDouble,\n        dayOfWeek.toDouble,\n        get_hour(crsDepTime).toDouble,\n        distance.toDouble,\n        days_from_nearest_holiday(year.toInt, month.toInt, dayOfMonth.toInt)\n      )\n      new Tuple2(to_date(year.toInt, month.toInt, dayOfMonth.toInt), values)\n    }\n\n    def get_hour(depTime: String) : String = \"%04d\".format(depTime.toInt).take(2)\n    def to_date(year: Int, month: Int, day: Int) = \"%04d%02d%02d\".format(year, month, day)\n\n    def days_from_nearest_holiday(year:Int, month:Int, day:Int): Int = {\n      val sampleDate = new DateTime(year, month, day, 0, 0)\n\n      holidays.foldLeft(3000) { (r, c) =>\n        val holiday = DateTimeFormat.forPattern(\"MM/dd/yyyy\").parseDateTime(c)\n        val distance = Math.abs(Days.daysBetween(holiday, sampleDate).getDays)\n        math.min(r, distance)\n      }\n    }\n  }\n\n// function to do a preprocessing step for a given filedd\ndef prepFlightDelays(infile: String): RDD[DelayRec] = {\n    val data = sc.textFile(infile)\n\n    data.map { line =>\n      val reader = new CSVReader(new StringReader(line))\n      reader.readAll().asScala.toList.map(rec => DelayRec(rec(0),rec(1),rec(2),rec(3),rec(5),rec(15),rec(16),rec(18),rec(21)))\n    }.map(list => list(0))\n    .filter(rec => rec.year != \"Year\")\n    .filter(rec => rec.cancelled == \"0\")\n    .filter(rec => rec.origin == \"ORD\")\n}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def parse(r):\n    try:\n        x=Row(Year=int(r[0]),\\\n          Month=int(r[1]),\\\n          DayofMonth=int(r[2]),\\\n          DayOfWeek=int(r[3]),\\\n          DepTime=int(float(r[4])), \\\n          CRSDepTime=int(r[5]),\\\n          ArrTime=int(float(r[6])),\\\n          CRSArrTime=int(r[7]), \\\n          UniqueCarrier=r[8],\\\n          DepDelay=int(float(r[15])),\\\n          Origin=r[16],\\\n          Dest=r[17], \\\n          Distance=int(float(r[18])),\\\n          CarrierDelay=int(float(r[24])),\\\n          WeatherDelay=int(float(r[25])),\\\n          NASDelay= int(float(r[26])),\\\n          SecurityDelay=int(float(r[27])),\\\n          LateAircraftDelay=int(float(r[28])))  \n    except:\n        x=None  \n    return x\n\nrowRDD = textRDD.map(lambda r: parse(r)).filter(lambda r:r != None)\nairline_df = sqlContext.createDataFrame(rowRDD)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}